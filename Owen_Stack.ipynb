{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 天池贷款违约预测竞赛 - Stacking Ensemble (进阶版)\n",
    "\n",
    "\n",
    "\n",
    " **竞赛链接**：[天池-零基础入门金融风控-贷款违约预测](https://tianchi.aliyun.com/competition/entrance/531830/information)\n",
    "\n",
    "\n",
    "\n",
    " **核心策略**：Stacking 集成\n",
    "\n",
    "\n",
    "\n",
    "目标是利用不同模型的优势，通过“取长补短”来提升预测精度。\n",
    "\n",
    "\n",
    "\n",
    " * **Level 0 (专家团)**：负责从原始数据中提取规律。\n",
    "\n",
    "     * **LightGBM**: 梯度提升树，训练速度快，精度高。\n",
    "\n",
    "     * **XGBoost**: 老牌强者，稳健性极佳。\n",
    "\n",
    "     * **CatBoost**: 擅长处理类别特征，与其他两个树模型差异性较大。\n",
    "\n",
    " * **Level 1 (裁判员)**：负责综合专家的意见。\n",
    "\n",
    "     * **Logistic Regression (LR)**: 简单的线性模型，用来给三个专家的预测结果加权。\n",
    "\n",
    "\n",
    "\n",
    " **主要流程**：\n",
    "\n",
    " 1.  **数据准备**：读取并预处理数据。\n",
    "\n",
    " 2.  **第一层训练 (Level 0)**：三位专家分别进行 5折交叉验证，生成“元特征”(Meta-features)。\n",
    "\n",
    " 3.  **相关性分析**：观察三位专家的意见是否一致（意见越不一致，融合效果往往越好）。\n",
    "\n",
    " 4.  **第二层训练 (Level 1)**：LR 根据元特征训练，得到最终的加权策略。\n",
    "\n",
    " 5.  **结果输出**：生成提交文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# 设置显示选项：显示所有列，方便查看数据\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. 数据读取与预处理\n",
    "\n",
    "\n",
    "\n",
    " 这里复用了之前验证过的稳健预处理逻辑。我们先合并训练集和测试集，统一处理完后再拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据...\n",
      "训练集: ./train.csv\n",
      "测试集: ./testA.csv\n",
      "原始训练集形状: (800000, 47)\n",
      "原始测试集形状: (200000, 46)\n",
      "\n",
      "数据预处理完成！\n",
      "最终训练集特征维度: (800000, 45)\n",
      "最终测试集特征维度: (200000, 45)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    print(f\"正在读取数据...\\n训练集: {train_path}\\n测试集: {test_path}\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    print(f\"原始训练集形状: {train_df.shape}\")\n",
    "    print(f\"原始测试集形状: {test_df.shape}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "# 请确保当前目录下有这两个文件\n",
    "TRAIN_FILE = \"./train.csv\"\n",
    "TEST_FILE = \"./testA.csv\"\n",
    "\n",
    "train_data, test_data = load_data(TRAIN_FILE, TEST_FILE)\n",
    "\n",
    "# 提取并分离标签\n",
    "target = train_data['isDefault']\n",
    "train_data = train_data.drop(['isDefault'], axis=1)\n",
    "\n",
    "# 合并数据\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# --- 缺失值填充 ---\n",
    "# 1. 离散/文本型 -> 众数填充\n",
    "# employmentLength 缺失通常代表无工作，填0\n",
    "data['employmentLength'] = data['employmentLength'].fillna(0)\n",
    "for col in ['employmentTitle', 'postCode', 'title']:\n",
    "    if data[col].isnull().sum() > 0:\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "# 2. 连续数值型 -> 均值填充\n",
    "for col in ['dti', 'pubRecBankruptcies', 'revolUtil', 'n11', 'n12']:\n",
    "    if data[col].isnull().sum() > 0:\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "# 3. 剩余匿名特征 -> 众数填充\n",
    "numerical_features = list(data.select_dtypes(exclude=['object']).columns)\n",
    "no_name_list = [col for col in numerical_features if col.startswith(\"n\") and data[col].isnull().sum() > 0]\n",
    "for col in no_name_list:\n",
    "    data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "# --- 特征工程 ---\n",
    "# 1. 处理工作年限 (正则提取数字)\n",
    "def employment_length_to_int(s):\n",
    "    if pd.isnull(s) or str(s) == 'nan': return 0\n",
    "    s = str(s).strip()\n",
    "    if '< 1' in s: return 0\n",
    "    numbers = re.findall(r'\\d+', s)\n",
    "    return int(numbers[0]) if numbers else 0\n",
    "\n",
    "data['employmentLength'] = data['employmentLength'].apply(employment_length_to_int)\n",
    "\n",
    "# 2. 处理最早信用记录 (提取年份)\n",
    "data['earliesCreditLine'] = data['earliesCreditLine'].apply(lambda s: int(str(s)[-4:]))\n",
    "\n",
    "# 3. 处理等级 (Label Encoding)\n",
    "for col in ['grade', 'subGrade']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "# 4. 删除日期列 (暂不使用)\n",
    "if 'issueDate' in data.columns:\n",
    "    data = data.drop(['issueDate'], axis=1)\n",
    "\n",
    "# --- 重新拆分数据 ---\n",
    "train_features = data[:len(target)]\n",
    "test_features = data[len(target):]\n",
    "\n",
    "print(\"\\n数据预处理完成！\")\n",
    "print(f\"最终训练集特征维度: {train_features.shape}\")\n",
    "print(f\"最终测试集特征维度: {test_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. 构建 Level 0：专家团 (Base Learners)\n",
    "\n",
    "\n",
    "\n",
    " 我们定义一个通用的 `get_oof` 函数。这个函数非常关键，它解决了不同模型库（Library）在最新版本中 API 不兼容的问题。\n",
    "\n",
    "\n",
    "\n",
    " * **LightGBM (v4.0+)**: 必须使用 `callbacks=[...]` 来控制早停。\n",
    "\n",
    " * **XGBoost (v2.0+)**: 必须在初始化时传入 `early_stopping_rounds`，不能在 fit 中传。\n",
    "\n",
    " * **CatBoost**: 依然支持在 fit 中传入 `early_stopping_rounds`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2023)\n",
    "\n",
    "def get_oof(clf, x_train, y_train, x_test, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    通用函数：执行 K折交叉验证，获取 Out-Of-Fold 预测值。\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 专家 [{name}] 开始诊断... ===\")\n",
    "    \n",
    "    # oof_train: 存放训练集每一折作为验证集时的预测结果 (长度 = 训练集行数)\n",
    "    oof_train = np.zeros((len(x_train),))\n",
    "    # oof_test_skf: 存放每一折模型对测试集的预测结果 (长度 = 测试集行数 x 折数)\n",
    "    oof_test_skf = np.zeros((len(x_test), nfold))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(x_train, y_train)):\n",
    "        # 切分数据\n",
    "        kf_x_train = x_train.iloc[train_index]\n",
    "        kf_y_train = y_train.iloc[train_index]\n",
    "        kf_x_test = x_train.iloc[test_index]\n",
    "        kf_y_test = y_train.iloc[test_index]\n",
    "        \n",
    "        # 获取模型类型，用于区分不同的 API 调用方式\n",
    "        model_type = str(type(clf))\n",
    "        \n",
    "        # --- 1. LightGBM 处理逻辑 ---\n",
    "        if 'LGBMClassifier' in model_type:\n",
    "            callbacks = [lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0)]\n",
    "            clf.fit(\n",
    "                kf_x_train, kf_y_train, \n",
    "                eval_set=[(kf_x_test, kf_y_test)], \n",
    "                eval_metric='auc', \n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            \n",
    "        # --- 2. XGBoost 处理逻辑 ---\n",
    "        elif 'XGBClassifier' in model_type:\n",
    "            # XGBoost 2.0+ 早停参数必须在初始化时设置，此处只负责 fit\n",
    "            clf.fit(\n",
    "                kf_x_train, kf_y_train, \n",
    "                eval_set=[(kf_x_test, kf_y_test)], \n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "        # --- 3. CatBoost 处理逻辑 ---\n",
    "        else:\n",
    "            clf.fit(\n",
    "                kf_x_train, kf_y_train, \n",
    "                eval_set=[(kf_x_test, kf_y_test)], \n",
    "                verbose=False, \n",
    "                early_stopping_rounds=50\n",
    "            )\n",
    "        \n",
    "        # 预测当前折的验证集\n",
    "        val_preds = clf.predict_proba(kf_x_test)[:,1]\n",
    "        oof_train[test_index] = val_preds\n",
    "        \n",
    "        # 预测测试集\n",
    "        oof_test_skf[:,i] = clf.predict_proba(x_test)[:,1]\n",
    "        \n",
    "        # 计算并打印当前折的 AUC 分数\n",
    "        fold_auc = roc_auc_score(kf_y_test, val_preds)\n",
    "        print(f\"Fold {i+1}/{nfold} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "    # 计算整体 CV 分数\n",
    "    all_auc = roc_auc_score(y_train, oof_train)\n",
    "    print(f\"专家 [{name}] 5折验证总 AUC: {all_auc:.5f}\")\n",
    "    print(f\"耗时: {time.time() - start_time:.2f} 秒\")\n",
    "    \n",
    "    # 测试集结果取平均\n",
    "    oof_test = oof_test_skf.mean(axis=1)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1 训练专家 A：LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 专家 [LightGBM] 开始诊断... ===\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[898]\tvalid_0's auc: 0.731468\n",
      "Fold 1/5 AUC: 0.73147\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1053]\tvalid_0's auc: 0.72787\n",
      "Fold 2/5 AUC: 0.72787\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1000]\tvalid_0's auc: 0.731249\n",
      "Fold 3/5 AUC: 0.73125\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1066]\tvalid_0's auc: 0.729213\n",
      "Fold 4/5 AUC: 0.72921\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1050]\tvalid_0's auc: 0.729796\n",
      "Fold 5/5 AUC: 0.72980\n",
      "专家 [LightGBM] 5折验证总 AUC: 0.72991\n",
      "耗时: 119.12 秒\n"
     ]
    }
   ],
   "source": [
    "clf_lgb = lgb.LGBMClassifier(\n",
    "    num_leaves=31, max_depth=-1, learning_rate=0.05, n_estimators=2000,\n",
    "    objective='binary', metric='auc', verbose=-1, random_state=2023\n",
    ")\n",
    "lgb_train, lgb_test = get_oof(clf_lgb, train_features, target, test_features, name=\"LightGBM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2 训练专家 B：XGBoost\n",
    "\n",
    " **注意**：针对 XGBoost 2.0+，`early_stopping_rounds` 必须在初始化时传入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 专家 [XGBoost] 开始诊断... ===\n",
      "Fold 1/5 AUC: 0.73323\n",
      "Fold 2/5 AUC: 0.72879\n",
      "Fold 3/5 AUC: 0.73269\n",
      "Fold 4/5 AUC: 0.72983\n",
      "Fold 5/5 AUC: 0.73132\n",
      "专家 [XGBoost] 5折验证总 AUC: 0.73116\n",
      "耗时: 174.35 秒\n"
     ]
    }
   ],
   "source": [
    "clf_xgb = xgb.XGBClassifier(\n",
    "    max_depth=6, learning_rate=0.05, n_estimators=2000,\n",
    "    objective='binary:logistic', eval_metric='auc',\n",
    "    tree_method='hist',         # 使用直方图加速\n",
    "    early_stopping_rounds=50,   # 早停策略放在这里\n",
    "    random_state=2023, n_jobs=-1\n",
    ")\n",
    "xgb_train, xgb_test = get_oof(clf_xgb, train_features, target, test_features, name=\"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.3 训练专家 C：CatBoost\n",
    "\n",
    " CatBoost 对类别特征处理独特，通常能提供差异化的预测视角。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 专家 [CatBoost] 开始诊断... ===\n",
      "Fold 1/5 AUC: 0.73082\n",
      "Fold 2/5 AUC: 0.72714\n",
      "Fold 3/5 AUC: 0.73160\n",
      "Fold 4/5 AUC: 0.72900\n",
      "Fold 5/5 AUC: 0.72993\n",
      "专家 [CatBoost] 5折验证总 AUC: 0.72969\n",
      "耗时: 222.12 秒\n"
     ]
    }
   ],
   "source": [
    "clf_cat = CatBoostClassifier(\n",
    "    iterations=1000, learning_rate=0.05, depth=6,\n",
    "    eval_metric='AUC', verbose=False, random_state=2023,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "cat_train, cat_test = get_oof(clf_cat, train_features, target, test_features, name=\"CatBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. 构建元特征 (Meta-features) 与相关性分析\n",
    "\n",
    "\n",
    "\n",
    " 我们将三位专家的预测结果拼接成一张新的表格。这张表只有3列，每一列代表一个专家的看法。\n",
    "\n",
    "\n",
    "\n",
    " **为什么看相关性？**\n",
    "\n",
    " 如果三个模型预测结果一模一样（相关性=1.0），那融合就没有意义了。相关性越低，说明模型关注的数据侧面不同，互补性越强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 元特征 (Meta-features) 前5行预览 ===\n",
      "   LightGBM   XGBoost  CatBoost\n",
      "0  0.313510  0.268663  0.308155\n",
      "1  0.281933  0.265492  0.251009\n",
      "2  0.371695  0.401617  0.399574\n",
      "3  0.052989  0.051469  0.063408\n",
      "4  0.352887  0.338223  0.356561\n",
      "\n",
      "=== 模型预测结果相关性矩阵 (Correlation Matrix) ===\n",
      "          LightGBM   XGBoost  CatBoost\n",
      "LightGBM  1.000000  0.982753  0.984224\n",
      "XGBoost   0.982753  1.000000  0.976763\n",
      "CatBoost  0.984224  0.976763  1.000000\n"
     ]
    }
   ],
   "source": [
    "# 拼接训练集元特征\n",
    "x_train_stack = np.concatenate((lgb_train, xgb_train, cat_train), axis=1)\n",
    "x_train_stack = pd.DataFrame(x_train_stack, columns=['LightGBM', 'XGBoost', 'CatBoost'])\n",
    "\n",
    "# 拼接测试集元特征\n",
    "x_test_stack = np.concatenate((lgb_test, xgb_test, cat_test), axis=1)\n",
    "x_test_stack = pd.DataFrame(x_test_stack, columns=['LightGBM', 'XGBoost', 'CatBoost'])\n",
    "\n",
    "print(\"\\n=== 元特征 (Meta-features) 前5行预览 ===\")\n",
    "print(x_train_stack.head())\n",
    "\n",
    "print(\"\\n=== 模型预测结果相关性矩阵 (Correlation Matrix) ===\")\n",
    "# 观察模型之间的相似度，越低越好（通常 0.9 以上都很高了，但只要不是 1.0 就有提升空间）\n",
    "print(x_train_stack.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Level 1：裁判员 (LR Meta-learner)\n",
    "\n",
    "\n",
    "\n",
    " 使用逻辑回归（LR）作为裁判。\n",
    "\n",
    " LR 会根据元特征学习一个公式：\n",
    "\n",
    " $$Final\\_Score = w_1 \\cdot LGB + w_2 \\cdot XGB + w_3 \\cdot Cat + Bias$$\n",
    "\n",
    "\n",
    "\n",
    " * **w (权重)**：如果某个专家特别准，LR 会给他更高的权重。\n",
    "\n",
    " * **Bias (截距)**：用于修正整体概率的偏移。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 裁判员 (LR) 开始综合意见... ===\n",
      "\n",
      "=== 最终权重分配 (裁判的决定) ===\n",
      "Bias (基础偏置): -2.7471\n",
      "LightGBM 权重: 1.9781\n",
      "XGBoost  权重: 2.0545\n",
      "CatBoost 权重: 1.9357\n",
      "\n",
      ">>> Stacking 融合后最终 CV AUC: 0.73152 <<<\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 裁判员 (LR) 开始综合意见... ===\")\n",
    "\n",
    "# 初始化 LR (不需要标准化，因为输入的概率值本身就在 0-1 之间且尺度一致)\n",
    "meta_learner = LogisticRegression(random_state=2023)\n",
    "meta_learner.fit(x_train_stack, target)\n",
    "\n",
    "# 获取权重\n",
    "weights = meta_learner.coef_[0]\n",
    "bias = meta_learner.intercept_[0]\n",
    "\n",
    "print(\"\\n=== 最终权重分配 (裁判的决定) ===\")\n",
    "print(f\"Bias (基础偏置): {bias:.4f}\")\n",
    "print(f\"LightGBM 权重: {weights[0]:.4f}\")\n",
    "print(f\"XGBoost  权重: {weights[1]:.4f}\")\n",
    "print(f\"CatBoost 权重: {weights[2]:.4f}\")\n",
    "\n",
    "# 生成最终预测\n",
    "final_preds = meta_learner.predict_proba(x_test_stack)[:, 1]\n",
    "\n",
    "# 计算 Stacking 在训练集上的表现\n",
    "train_preds_final = meta_learner.predict_proba(x_train_stack)[:, 1]\n",
    "score = roc_auc_score(target, train_preds_final)\n",
    "\n",
    "print(f\"\\n>>> Stacking 融合后最终 CV AUC: {score:.5f} <<<\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "任务完成！提交文件已保存至: submit_stack.csv\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "# 检查是否有 ID 列，如果没有则使用索引\n",
    "submission['id'] = test_data['id'] if 'id' in test_data.columns else range(len(final_preds))\n",
    "submission['isDefault'] = final_preds\n",
    "\n",
    "output_file = 'submit_stack.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"任务完成！提交文件已保存至: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
